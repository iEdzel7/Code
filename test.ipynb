{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved at: Code Readability\\Dataset\\scores_updated.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_19816\\2846037612.py:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  file_path = \"Code Readability\\Dataset\\scores.csv\"  # Change this to the actual file path\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_19816\\2846037612.py:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  updated_file_path = \"Code Readability\\Dataset\\scores_updated.csv\"  # Change this to desired save location\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"Code Readability\\Dataset\\scores.csv\"  # Change this to the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove the first column\n",
    "df = df.iloc[:, 1:]\n",
    "\n",
    "# Compute the mean for each \"Snippet\" column\n",
    "df_mean = df.mean(axis=0)\n",
    "\n",
    "# Convert the mean values to a single-row DataFrame\n",
    "df = pd.DataFrame([df_mean])\n",
    "\n",
    "# Save the updated CSV file\n",
    "updated_file_path = \"Code Readability\\Dataset\\scores_updated.csv\"  # Change this to desired save location\n",
    "df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV saved at: {updated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CSV saved at: Code Readability\\Dataset\\scores_final.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_19816\\2142069152.py:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  file_path = \"Code Readability\\Dataset\\scores_updated.csv\"  # Change this to the actual file path\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_19816\\2142069152.py:14: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  updated_file_path = \"Code Readability\\Dataset\\scores_final.csv\"  # Change this to desired save location\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"Code Readability\\Dataset\\scores_updated.csv\"  # Change this to the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Transpose the DataFrame to convert columns into rows\n",
    "df = df.T.reset_index()\n",
    "\n",
    "# Rename columns\n",
    "df.columns = [\"Code Snippet\", \"Code Readability Score\"]\n",
    "\n",
    "# Save the modified CSV file\n",
    "updated_file_path = \"Code Readability\\Dataset\\scores_final.csv\"  # Change this to desired save location\n",
    "df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"Final CSV saved at: {updated_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_19816\\2870752706.py:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  file_path = \"Code Readability\\Dataset\\scores_final.csv\"  # Change this to the actual file path\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_19816\\2870752706.py:9: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  snippets_folder = \"Code Readability\\Dataset\\Snippets\"  # Change this to the actual folder path\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_19816\\2870752706.py:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  updated_file_path = \"Code Readability\\snippets_updated.csv\"  # Change this to desired save location\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved at: Code Readability\\snippets_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"Code Readability\\Dataset\\scores_final.csv\"  # Change this to the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the folder containing the .jsnp files\n",
    "snippets_folder = \"Code Readability\\Dataset\\Snippets\"  # Change this to the actual folder path\n",
    "\n",
    "# Read each .jsnp file and replace the corresponding row in the CSV\n",
    "for i in range(1, 201):\n",
    "    snippet_path = os.path.join(snippets_folder, f\"{i}.jsnp\")\n",
    "    \n",
    "    if os.path.exists(snippet_path):\n",
    "        with open(snippet_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            code_content = file.read()\n",
    "        \n",
    "        # Replace the corresponding \"Code Snippet\" value in the DataFrame\n",
    "        df.loc[df[\"Code Snippet\"] == f\"Snippet{i}\", \"Code Snippet\"] = code_content\n",
    "    \n",
    "# Save the updated CSV file\n",
    "updated_file_path = \"Code Readability\\snippets_updated.csv\"  # Change this to desired save location\n",
    "df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV saved at: {updated_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved at: C:\\Users\\Edzel Armengol/Desktop//GITHUB/Streamlit/Code Readability/HugginFace/code-readability-krod.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"se2p/code-readability-krod\")\n",
    "\n",
    "# Extract the 'train' set since there's no separate test/validation split\n",
    "ds = ds['train']\n",
    "\n",
    "# Convert dataset to list and then to Pandas DataFrame\n",
    "df = pd.DataFrame(ds.to_list())\n",
    "\n",
    "# Remove the 'name' column if it exists\n",
    "if 'name' in df.columns:\n",
    "    df = df.drop(columns=['name'])\n",
    "\n",
    "# Define the save location\n",
    "save_path = os.path.expanduser(\"~/Desktop//GITHUB/Streamlit/Code Readability/HugginFace/code-readability-krod.csv\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved at: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved at: C:\\Users\\Edzel Armengol/Desktop//GITHUB/Streamlit/code_readability_merged.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"se2p/code-readability-merged\")\n",
    "\n",
    "# Extract the 'train' set since there's no separate test/validation split\n",
    "ds = ds['train']\n",
    "\n",
    "# Convert dataset to list and then to Pandas DataFrame\n",
    "df = pd.DataFrame(ds.to_list())\n",
    "\n",
    "# Define the save location\n",
    "save_path = os.path.expanduser(\"~/Desktop//GITHUB/Streamlit/code_readability_merged.csv\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved at: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final combined CSV saved at: Code Readability/Final_CodeReadability.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "file1 = \"Code Readability/snippets_updated.csv\"  # First dataset\n",
    "file2 = \"Code Readability/HugginFace/code_readability_merged.csv\"  # Second dataset\n",
    "output_file = \"Code Readability/Final_CodeReadability.csv\"  # Output file\n",
    "\n",
    "# Load datasets\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "# Rename columns in df2 and df3 to match df1\n",
    "df2 = df2.rename(columns={\"code_snippet\": \"Code Snippet\", \"score\": \"Code Readability Score\"})\n",
    "\n",
    "# Select only necessary columns\n",
    "df2 = df2[[\"Code Snippet\", \"Code Readability Score\"]]\n",
    "\n",
    "\n",
    "# Combine datasets\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the final combined dataset\n",
    "df_combined.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Final combined CSV saved at: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the dataset path\n",
    "csv_path = os.path.expanduser(\"~/Desktop/GITHUB/Streamlit/BeetleBox.csv\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Filter only Java-related rows\n",
    "df = df[df[\"language\"].str.lower() == \"java\"]\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_remove = [\n",
    "    \"commit_datetime\", \"report_datetime\", \"issue_url\", \n",
    "    \"pull_url\", \"repo_url\", \"repo_name\", \"status\", \"before_fix_sha\", \"after_fix_sha\"\n",
    "]\n",
    "df = df.drop(columns=columns_to_remove, errors=\"ignore\")\n",
    "\n",
    "# Save the cleaned dataset back\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Filtered and cleaned dataset saved at: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edzel Armengol\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Edzel Armengol\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.5513 - loss: 0.6857 - val_accuracy: 0.4960 - val_loss: 0.6791\n",
      "Epoch 2/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.5748 - loss: 0.6249 - val_accuracy: 0.6240 - val_loss: 0.6202\n",
      "Epoch 3/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.8259 - loss: 0.3960 - val_accuracy: 0.7440 - val_loss: 0.7612\n",
      "Epoch 4/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9239 - loss: 0.1762 - val_accuracy: 0.7920 - val_loss: 0.7573\n",
      "Epoch 5/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9842 - loss: 0.0436 - val_accuracy: 0.8400 - val_loss: 1.3014\n",
      "Epoch 6/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9974 - loss: 0.0079 - val_accuracy: 0.8240 - val_loss: 1.6865\n",
      "Epoch 7/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.8160 - val_loss: 1.5828\n",
      "Epoch 8/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 9.1115e-04 - val_accuracy: 0.8160 - val_loss: 1.6765\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8327 - loss: 1.3581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout, LeakyReLU\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Code Readability/Final_CodeReadability.csv\")\n",
    "\n",
    "# Assume 'Code Snippet' column contains the source code snippets and 'Code Readability Score' is the numerical readability score\n",
    "X = df['Code Snippet'].astype(str)  # Convert to string if not already\n",
    "y = df['Code Readability Score']  # Readability scores\n",
    "\n",
    "# Normalize scores to binary classification (Readable if score > median, else Unreadable)\n",
    "median_score = y.median()\n",
    "y = (y > median_score).astype(int)\n",
    "\n",
    "# Tokenization\n",
    "max_words = 10000  # Vocabulary size\n",
    "max_len = 200  # Max sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN Model\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "    layers.Conv1D(128, 5, activation='relu'),\n",
    "    layers.MaxPooling1D(2),\n",
    "    Dropout(0.3),  # Prevent overfitting\n",
    "    layers.Conv1D(64, 5),\n",
    "    LeakyReLU(alpha=0.01),  # Apply LeakyReLU activation\n",
    "    layers.MaxPooling1D(2),\n",
    "    Dropout(0.3),  # Prevent overfitting\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64),\n",
    "    LeakyReLU(alpha=0.01),  # Apply LeakyReLU activation\n",
    "    Dropout(0.3),  # Prevent overfitting\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=8, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"code_readability_model.h5\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the tokenizer after training\n",
    "with open(\"tokenizer.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step \n",
      "Test Accuracy: 0.82\n",
      "Precision: 0.84\n",
      "Recall: 0.77\n",
      "F1 Score: 0.80\n",
      "Confusion Matrix:\n",
      "[[55  9]\n",
      " [14 47]]\n",
      "Valid input. Processing...\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION METRICS\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# VALIDATION CHECKS\n",
    "def validate_model_input(java_code):\n",
    "    \"\"\"Validate user input before processing.\"\"\"\n",
    "    if len(java_code.strip()) == 0:\n",
    "        return \"Error: No Java code entered. Please enter a valid Java code snippet.\"\n",
    "    return \"Valid input. Processing...\"\n",
    "\n",
    "# Example Usage\n",
    "java_code = \"public class Test { public static void main(String[] args) { System.out.println(\\\"Hello\\\"); } }\"\n",
    "print(validate_model_input(java_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 333ms/step\n",
      "Predicted Readability Score: 5/5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"code_readability_model.h5\")\n",
    "\n",
    "# Load tokenizer\n",
    "with open(\"tokenizer.pkl\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Tokenization settings\n",
    "max_len = 200  # Same max sequence length used during training\n",
    "\n",
    "def predict_readability(java_code):\n",
    "    \"\"\"Predict readability score for a given Java code snippet.\"\"\"\n",
    "    sequence = tokenizer.texts_to_sequences([java_code])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')\n",
    "    \n",
    "    prediction = model.predict(padded_sequence)[0][0]  # Get prediction value (0 to 1)\n",
    "\n",
    "    # Convert binary probability back to 1-5 scale\n",
    "    score = round(prediction * 4 + 1)  # Scale from (0-1) to (1-5)\n",
    "    \n",
    "    return f\"Predicted Readability Score: {score}/5\"\n",
    "\n",
    "# Get user input\n",
    "java_code = input(\"Enter your Java code snippet: \")\n",
    "\n",
    "# Predict readability score\n",
    "print(predict_readability(java_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
