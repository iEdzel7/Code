{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_18536\\2846037612.py:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  file_path = \"Code Readability\\Dataset\\scores.csv\"  # Change this to the actual file path\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_18536\\2846037612.py:17: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  updated_file_path = \"Code Readability\\Dataset\\scores_updated.csv\"  # Change this to desired save location\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved at: Code Readability\\Dataset\\scores_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"Code Readability\\Dataset\\scores.csv\"  # Change this to the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove the first column\n",
    "df = df.iloc[:, 1:]\n",
    "\n",
    "# Compute the mean for each \"Snippet\" column\n",
    "df_mean = df.mean(axis=0)\n",
    "\n",
    "# Convert the mean values to a single-row DataFrame\n",
    "df = pd.DataFrame([df_mean])\n",
    "\n",
    "# Save the updated CSV file\n",
    "updated_file_path = \"Code Readability\\Dataset\\scores_updated.csv\"  # Change this to desired save location\n",
    "df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV saved at: {updated_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final CSV saved at: Code Readability\\Dataset\\scores_final.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:14: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_18536\\2142069152.py:4: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  file_path = \"Code Readability\\Dataset\\scores_updated.csv\"  # Change this to the actual file path\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_18536\\2142069152.py:14: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  updated_file_path = \"Code Readability\\Dataset\\scores_final.csv\"  # Change this to desired save location\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"Code Readability\\Dataset\\scores_updated.csv\"  # Change this to the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Transpose the DataFrame to convert columns into rows\n",
    "df = df.T.reset_index()\n",
    "\n",
    "# Rename columns\n",
    "df.columns = [\"Code Snippet\", \"Code Readability Score\"]\n",
    "\n",
    "# Save the modified CSV file\n",
    "updated_file_path = \"Code Readability\\Dataset\\scores_final.csv\"  # Change this to desired save location\n",
    "df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"Final CSV saved at: {updated_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_18536\\2870752706.py:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  file_path = \"Code Readability\\Dataset\\scores_final.csv\"  # Change this to the actual file path\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_18536\\2870752706.py:9: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  snippets_folder = \"Code Readability\\Dataset\\Snippets\"  # Change this to the actual folder path\n",
      "C:\\Users\\Edzel Armengol\\AppData\\Local\\Temp\\ipykernel_18536\\2870752706.py:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  updated_file_path = \"Code Readability\\snippets_updated.csv\"  # Change this to desired save location\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved at: Code Readability\\snippets_updated.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"Code Readability\\Dataset\\scores_final.csv\"  # Change this to the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the folder containing the .jsnp files\n",
    "snippets_folder = \"Code Readability\\Dataset\\Snippets\"  # Change this to the actual folder path\n",
    "\n",
    "# Read each .jsnp file and replace the corresponding row in the CSV\n",
    "for i in range(1, 201):\n",
    "    snippet_path = os.path.join(snippets_folder, f\"{i}.jsnp\")\n",
    "    \n",
    "    if os.path.exists(snippet_path):\n",
    "        with open(snippet_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            code_content = file.read()\n",
    "        \n",
    "        # Replace the corresponding \"Code Snippet\" value in the DataFrame\n",
    "        df.loc[df[\"Code Snippet\"] == f\"Snippet{i}\", \"Code Snippet\"] = code_content\n",
    "    \n",
    "# Save the updated CSV file\n",
    "updated_file_path = \"Code Readability\\snippets_updated.csv\"  # Change this to desired save location\n",
    "df.to_csv(updated_file_path, index=False)\n",
    "\n",
    "print(f\"Updated CSV saved at: {updated_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved at: C:\\Users\\Edzel Armengol/Desktop//GITHUB/Streamlit/Code Readability/HugginFace/code-readability-krod.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"se2p/code-readability-krod\")\n",
    "\n",
    "# Extract the 'train' set since there's no separate test/validation split\n",
    "ds = ds['train']\n",
    "\n",
    "# Convert dataset to list and then to Pandas DataFrame\n",
    "df = pd.DataFrame(ds.to_list())\n",
    "\n",
    "# Remove the 'name' column if it exists\n",
    "if 'name' in df.columns:\n",
    "    df = df.drop(columns=['name'])\n",
    "\n",
    "# Define the save location\n",
    "save_path = os.path.expanduser(\"~/Desktop//GITHUB/Streamlit/Code Readability/HugginFace/code-readability-krod.csv\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved at: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved at: C:\\Users\\Edzel Armengol/Desktop//GITHUB/Streamlit/code_readability_merged.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "ds = load_dataset(\"se2p/code-readability-merged\")\n",
    "\n",
    "# Extract the 'train' set since there's no separate test/validation split\n",
    "ds = ds['train']\n",
    "\n",
    "# Convert dataset to list and then to Pandas DataFrame\n",
    "df = pd.DataFrame(ds.to_list())\n",
    "\n",
    "# Define the save location\n",
    "save_path = os.path.expanduser(\"~/Desktop//GITHUB/Streamlit/code_readability_merged.csv\")\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv(save_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved at: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final combined CSV saved at: Code Readability/Final_CodeReadability.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "file1 = \"Code Readability/snippets_updated.csv\"  # First dataset\n",
    "file2 = \"Code Readability/HugginFace/code_readability_merged.csv\"  # Second dataset\n",
    "output_file = \"Code Readability/Final_CodeReadability.csv\"  # Output file\n",
    "\n",
    "# Load datasets\n",
    "df1 = pd.read_csv(file1)\n",
    "df2 = pd.read_csv(file2)\n",
    "\n",
    "# Rename columns in df2 and df3 to match df1\n",
    "df2 = df2.rename(columns={\"code_snippet\": \"Code Snippet\", \"score\": \"Code Readability Score\"})\n",
    "\n",
    "# Select only necessary columns\n",
    "df2 = df2[[\"Code Snippet\", \"Code Readability Score\"]]\n",
    "\n",
    "\n",
    "# Combine datasets\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the final combined dataset\n",
    "df_combined.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Final combined CSV saved at: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edzel Armengol\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Edzel Armengol\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.5455 - loss: 0.6946 - val_accuracy: 0.5120 - val_loss: 0.6833\n",
      "Epoch 2/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.5529 - loss: 0.6559 - val_accuracy: 0.5680 - val_loss: 0.6529\n",
      "Epoch 3/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.6909 - loss: 0.5379 - val_accuracy: 0.6640 - val_loss: 0.5880\n",
      "Epoch 4/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.8790 - loss: 0.3240 - val_accuracy: 0.7600 - val_loss: 0.7582\n",
      "Epoch 5/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - accuracy: 0.9631 - loss: 0.0932 - val_accuracy: 0.7680 - val_loss: 0.7915\n",
      "Epoch 6/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9795 - loss: 0.0738 - val_accuracy: 0.7600 - val_loss: 0.8561\n",
      "Epoch 7/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9956 - loss: 0.0200 - val_accuracy: 0.7600 - val_loss: 1.0618\n",
      "Epoch 8/8\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.7520 - val_loss: 1.2188\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7675 - loss: 1.0443 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dropout, LeakyReLU\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Code Readability/Final_CodeReadability.csv\")\n",
    "\n",
    "# Assume 'Code Snippet' column contains the source code snippets and 'Code Readability Score' is the numerical readability score\n",
    "X = df['Code Snippet'].astype(str)  # Convert to string if not already\n",
    "y = df['Code Readability Score']  # Readability scores\n",
    "\n",
    "# Normalize scores to binary classification (Readable if score > median, else Unreadable)\n",
    "median_score = y.median()\n",
    "y = (y > median_score).astype(int)\n",
    "\n",
    "# Tokenization\n",
    "max_words = 10000  # Vocabulary size\n",
    "max_len = 200  # Max sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_sequences = tokenizer.texts_to_sequences(X)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build CNN Model\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
    "    layers.Conv1D(128, 5, activation='relu'),\n",
    "    layers.MaxPooling1D(2),\n",
    "    Dropout(0.3),  # Prevent overfitting\n",
    "    layers.Conv1D(64, 5),\n",
    "    LeakyReLU(alpha=0.01),  # Apply LeakyReLU activation\n",
    "    layers.MaxPooling1D(2),\n",
    "    Dropout(0.3),  # Prevent overfitting\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64),\n",
    "    LeakyReLU(alpha=0.01),  # Apply LeakyReLU activation\n",
    "    Dropout(0.3),  # Prevent overfitting\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=8, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"code_readability_model.h5\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the tokenizer after training\n",
    "with open(\"tokenizer.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Test Accuracy: 0.75\n",
      "Precision: 0.74\n",
      "Recall: 0.75\n",
      "F1 Score: 0.75\n",
      "Confusion Matrix:\n",
      "[[48 16]\n",
      " [15 46]]\n",
      "Valid input. Processing...\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION METRICS\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# VALIDATION CHECKS\n",
    "def validate_model_input(java_code):\n",
    "    \"\"\"Validate user input before processing.\"\"\"\n",
    "    if len(java_code.strip()) == 0:\n",
    "        return \"Error: No Java code entered. Please enter a valid Java code snippet.\"\n",
    "    return \"Valid input. Processing...\"\n",
    "\n",
    "# Example Usage\n",
    "java_code = \"public class Test { public static void main(String[] args) { System.out.println(\\\"Hello\\\"); } }\"\n",
    "print(validate_model_input(java_code))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💻 Enter your Java code snippet (Press Enter twice to finish):\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Readability Score: 3/5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import groq\n",
    "\n",
    "# Set up Groq API Key\n",
    "groq.api_key = \"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\"\n",
    "\n",
    "# Load trained readability model\n",
    "model = tf.keras.models.load_model(\"code_readability_model.h5\")\n",
    "\n",
    "# Load tokenizer\n",
    "with open(\"tokenizer.pkl\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Tokenization settings\n",
    "max_len = 200  # Same max sequence length used during training\n",
    "\n",
    "def predict_readability(java_code):\n",
    "    \"\"\"Predict numerical readability score using the trained ML model.\"\"\"\n",
    "    sequence = tokenizer.texts_to_sequences([java_code])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    # Get prediction from trained model\n",
    "    prediction = model.predict(padded_sequence)[0][0]\n",
    "\n",
    "    # Convert probability to 1-5 scale\n",
    "    score = round(prediction * 4 + 1)\n",
    "\n",
    "    return score\n",
    "\n",
    "def classify_readability_with_llama(java_code):\n",
    "    \"\"\"Use Groq's Llama 3 AI to classify code and assign a readability/efficiency score.\"\"\"\n",
    "    client = groq.Client(api_key=groq.api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \n",
    "                \"Analyze the following Java code snippet and provide a readability/efficiency score from 1-5. \"\n",
    "                \"Score the code **1 or 2** if it is difficult to read due to poor formatting, unclear variable names, lack of indentation, or compressed structure. \"\n",
    "                \"Score the code **4 or 5** if it follows good coding practices such as proper indentation, clear variable names, and spacing. \"\n",
    "                \"If the code is average but could use minor improvements, assign a score of **3**. \"\n",
    "                \"Return only the number (1, 2, 3, 4, or 5). Do not include any other text.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": java_code}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract the numerical score from Llama 3 response\n",
    "    llama_score_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    try:\n",
    "        llama_score = int(llama_score_text)\n",
    "        if 1 <= llama_score <= 5:\n",
    "            return llama_score\n",
    "    except ValueError:\n",
    "        return None  # If Llama 3 fails to return a valid number, ignore it\n",
    "\n",
    "def compute_final_score(ml_score, llama_score):\n",
    "    \"\"\"Combine the ML model score and Llama 3 AI score with a weighted approach.\"\"\"\n",
    "    if llama_score is None:\n",
    "        return ml_score  # If Llama 3 fails, rely on ML model only\n",
    "\n",
    "    # Weighted average: Llama 3 score influences 70%, ML model 30%\n",
    "    final_score = round((ml_score * 0.3) + (llama_score * 0.7))\n",
    "    \n",
    "    # Ensure score is between 1-5\n",
    "    return max(1, min(final_score, 5))\n",
    "\n",
    "# Get user input (multi-line support)\n",
    "print(\"\\n💻 Enter your Java code snippet (Press Enter twice to finish):\")\n",
    "java_code = \"\"\n",
    "while True:\n",
    "    try:\n",
    "        line = input()\n",
    "        if line.strip() == \"\":  # Stop input on empty line\n",
    "            break\n",
    "        java_code += line + \"\\n\"\n",
    "    except EOFError:\n",
    "        break\n",
    "\n",
    "# Predict readability score using ML model\n",
    "numerical_score = predict_readability(java_code)\n",
    "\n",
    "# Get readability classification & score from Groq Llama 3\n",
    "llama_score = classify_readability_with_llama(java_code)\n",
    "\n",
    "# Compute final weighted score\n",
    "final_score = compute_final_score(numerical_score, llama_score)\n",
    "print(f\"Readability Score: {final_score}/5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created successfully at: C:/Users/Edzel Armengol/Desktop/GITHUB/Code/combined_snippets.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Define the directories containing the Python files\n",
    "buggy_dir = \"C:/Users/Edzel Armengol/Desktop/GITHUB/Code/buggy_dataset/buggy_snippets_files\"\n",
    "stable_dir = \"C:/Users/Edzel Armengol/Desktop/GITHUB/Code/stable_snippets_files\"\n",
    "\n",
    "# Define the output CSV file path\n",
    "output_csv = \"C:/Users/Edzel Armengol/Desktop/GITHUB/Code/combined_snippets.csv\"\n",
    "\n",
    "# Function to read Python files from a directory\n",
    "def read_python_files(directory, category, limit):\n",
    "    data = []\n",
    "    count = 0\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.py'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    code = file.read()\n",
    "                    data.append([code, category])\n",
    "                    count += 1\n",
    "                    if count >= limit:\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {filename} due to error: {e}\")  # Log the file causing issues\n",
    "    return data\n",
    "\n",
    "# Read 10,000 buggy Python files\n",
    "buggy_data = read_python_files(buggy_dir, 'Buggy', 5000)\n",
    "\n",
    "# Read 10,000 stable Python files\n",
    "stable_data = read_python_files(stable_dir, 'Non-Buggy', 5000)\n",
    "\n",
    "# Combine the data\n",
    "combined_data = buggy_data + stable_data\n",
    "\n",
    "# Write the combined data to a CSV file\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Code\", \"Category\"])  # Write the header\n",
    "    writer.writerows(combined_data)  # Write the data\n",
    "\n",
    "print(f\"CSV file created successfully at: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer saved as 'BugTokenizer.pkl'\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edzel Armengol\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Edzel Armengol\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 142ms/step - accuracy: 0.5867 - loss: 0.6379 - val_accuracy: 0.6032 - val_loss: 0.5656\n",
      "Epoch 2/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 132ms/step - accuracy: 0.5913 - loss: 0.5650 - val_accuracy: 0.6032 - val_loss: 0.5623\n",
      "Epoch 3/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 132ms/step - accuracy: 0.6144 - loss: 0.5638 - val_accuracy: 0.6032 - val_loss: 0.5630\n",
      "Epoch 4/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 131ms/step - accuracy: 0.5940 - loss: 0.5478 - val_accuracy: 0.6032 - val_loss: 0.5618\n",
      "Epoch 5/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 131ms/step - accuracy: 0.6021 - loss: 0.5631 - val_accuracy: 0.6032 - val_loss: 0.5621\n",
      "Epoch 6/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 131ms/step - accuracy: 0.5731 - loss: 0.5713 - val_accuracy: 0.5873 - val_loss: 0.5642\n",
      "Epoch 7/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 131ms/step - accuracy: 0.5889 - loss: 0.5555 - val_accuracy: 0.5873 - val_loss: 0.5666\n",
      "Epoch 8/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 131ms/step - accuracy: 0.5889 - loss: 0.5671 - val_accuracy: 0.5873 - val_loss: 0.5643\n",
      "Epoch 9/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 135ms/step - accuracy: 0.5915 - loss: 0.5570 - val_accuracy: 0.5873 - val_loss: 0.5624\n",
      "Epoch 10/10\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 130ms/step - accuracy: 0.6069 - loss: 0.5606 - val_accuracy: 0.6032 - val_loss: 0.5618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved as 'bug_localization_model.h5'\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.5901 - loss: 0.5743\n",
      "Test Accuracy: 60.32%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle  # Import pickle for saving tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, LSTM, Dropout, Embedding, Flatten\n",
    "from tensorflow.keras.layers import LeakyReLU, ReLU, Activation\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset (Update the path)\n",
    "csv_path = \"C:/Users/Edzel Armengol/Desktop/GITHUB/Code/BugLocalization/combined_java_snippets.csv\"  # Update with actual path\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert 'Category' column to binary labels (Buggy = 1, Non-Buggy = 0)\n",
    "df[\"Category\"] = df[\"Category\"].apply(lambda x: 1 if x == \"Buggy\" else 0)\n",
    "\n",
    "# Tokenize Java code snippets (character-level tokenization)\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(df[\"Code\"])\n",
    "\n",
    "# Save tokenizer as 'BugTokenizer.pkl'\n",
    "with open(\"BugTokenizer.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"✅ Tokenizer saved as 'BugTokenizer.pkl'\")\n",
    "\n",
    "# Convert text to sequences and pad them\n",
    "X = tokenizer.texts_to_sequences(df[\"Code\"])\n",
    "X = pad_sequences(X, maxlen=500, padding=\"post\")  # Limit to 500 characters per snippet\n",
    "\n",
    "# Convert labels to NumPy array\n",
    "y = np.array(df[\"Category\"])\n",
    "\n",
    "# Split dataset into training & testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Bug Localization Model\n",
    "model = Sequential([\n",
    "    # Embedding layer to convert character sequences into dense vectors\n",
    "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=500),\n",
    "    \n",
    "    # CNN Layer for feature extraction\n",
    "    Conv1D(filters=64, kernel_size=3, padding=\"same\"),\n",
    "    LeakyReLU(alpha=0.1),  # Activation Function 1: LeakyReLU\n",
    "    \n",
    "    # LSTM Layer for sequence processing\n",
    "    LSTM(50, return_sequences=False),\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    Dense(128),\n",
    "    ReLU(),  # Activation Function 2: ReLU\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(64),\n",
    "    Activation(\"sigmoid\"),  # Activation Function 3: Sigmoid\n",
    "    \n",
    "    # Output Layer (Binary Classification: Buggy or Non-Buggy)\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"bug_localization_model.h5\")\n",
    "print(\"✅ Model saved as 'bug_localization_model.h5'\")\n",
    "\n",
    "# Evaluate Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💻 Enter your Java code snippet (Press Enter twice to finish):\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
      "\n",
      "🔍 Bug Localization Analysis:\n",
      "✅ Code is Non-Buggy!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import groq\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set up Groq API Key\n",
    "groq.api_key = \"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\"\n",
    "\n",
    "# Load trained bug detection model\n",
    "model = tf.keras.models.load_model(\"bug_localization_model.h5\")\n",
    "\n",
    "# Load tokenizer\n",
    "with open(\"BugTokenizer.pkl\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Tokenization settings\n",
    "max_len = 500  # Must match training settings\n",
    "\n",
    "def predict_bug_status_ml(java_code):\n",
    "    \"\"\"Predict if a Java code snippet is Buggy or Non-Buggy using ML model.\"\"\"\n",
    "    # Convert user input to a sequence\n",
    "    sequence = tokenizer.texts_to_sequences([java_code])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "    # Get ML model prediction (probability between 0-1)\n",
    "    prediction = model.predict(padded_sequence)[0][0]\n",
    "\n",
    "    return prediction  # Higher values mean more likely buggy\n",
    "\n",
    "def classify_bug_with_groq(java_code):\n",
    "    \"\"\"Use Groq's Llama 3 AI to determine if the code looks buggy.\"\"\"\n",
    "    client = groq.Client(api_key=groq.api_key)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \n",
    "                \"Analyze the following Java code snippet and determine if it looks like it is buggy or not. \"\n",
    "                \"If the code contains poor practices, missing conditions, potential runtime errors, or looks structurally incorrect, consider it buggy. \"\n",
    "                \"Otherwise, if the code follows good syntax and logic, consider it non-buggy. \"\n",
    "                \"Return a probability score between 0 and 1, where: \"\n",
    "                \"0 = Absolutely non-buggy, 1 = Absolutely buggy. \"\n",
    "                \"Return only the probability value. Do not include any other text.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": java_code}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract the probability from Groq response\n",
    "    groq_score_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    try:\n",
    "        groq_score = float(groq_score_text)\n",
    "        if 0 <= groq_score <= 1:\n",
    "            return groq_score\n",
    "    except ValueError:\n",
    "        return None  # If Groq fails, ignore its result\n",
    "\n",
    "def compute_final_bug_score(ml_score, groq_score):\n",
    "    \"\"\"Combine ML model and Groq's predictions, giving Groq 75% weight.\"\"\"\n",
    "    if groq_score is None:\n",
    "        return \"❌ Error: Groq response invalid. ML Score: \" + (\"⚠️ Buggy\" if ml_score > 0.5 else \"✅ Non-Buggy\")\n",
    "\n",
    "    # Weighted final decision (Groq 75%, ML 25%)\n",
    "    final_score = (ml_score * 0.25) + (groq_score * 0.75)\n",
    "\n",
    "    # Classify as Buggy or Non-Buggy\n",
    "    return \"⚠️ Buggy Code Detected!\" if final_score > 0.5 else \"✅ Code is Non-Buggy!\"\n",
    "\n",
    "# Get user input (multi-line support)\n",
    "print(\"\\n💻 Enter your Java code snippet (Press Enter twice to finish):\")\n",
    "java_code = \"\"\n",
    "while True:\n",
    "    try:\n",
    "        line = input()\n",
    "        if line.strip() == \"\":  # Stop input on empty line\n",
    "            break\n",
    "        java_code += line + \"\\n\"\n",
    "    except EOFError:\n",
    "        break\n",
    "\n",
    "# Predict bug status using both ML model and Groq\n",
    "ml_prediction = predict_bug_status_ml(java_code)\n",
    "groq_prediction = classify_bug_with_groq(java_code)\n",
    "\n",
    "# Compute final decision\n",
    "final_result = compute_final_bug_score(ml_prediction, groq_prediction)\n",
    "\n",
    "# Display result\n",
    "print(\"\\n🔍 Bug Localization Analysis:\")\n",
    "print(final_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
